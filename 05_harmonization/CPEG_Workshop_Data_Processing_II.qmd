---
title: "CPEG Workshop Afternoon Session"
format: html
editor: visual
author: 'Erin M. Dillon & Christopher D. Dean'
date: 2025-07-27
title-block-banner: true
description: 'Data Processing II: Harmonization and Synthesis'
---

## Learning Objectives

In this session, we'll walk through how to combine and use different deep-time and modern datasets to answer questions that cross the palaeo-ecological divide. We'll cover:

-  The power of harmonizing data from different sources
-  The considerations and complications that arise when combining datasets
-  How to source spatially explicit climatic data for the past and present, and how to look at relationships with fossil and modern occurrences
-  How to plan, write code for, and carry out projects covering multiple time frames

To do this, we'll continue working with the Crocodylia dataset. If you've not quite finished cleaning your dataset, you can find a cleaned version that's ready for this next step **HERE**.

## Plan for the Session

The session will be split into the following sections:

I.   Introduction to harmonization and synthesis
II.  Planning, setup, and finding your data
III. Building a workflow to merge datasets
IV.  Data visualisation and synthesis

## I. Introduction to Harmonization and Synthesis

Presentation goes here??

## II. Planning, Setup, and Finding Your Data

### Our Research Question

Our first step will be to define our research question, which will be framed around the dataset of Cenozoic Crocodylia that we've been working on this morning. As mentioned previously, Crocs are an excellent group to use for studies comparing modern and deep-time records: they have a very tight association with temperature, are well represented in the fossil record spatially and temporally, and many extant species are threatened by anthropogenic impacts including habitat loss and climate change.

In this section, we'll aim to answer the following research question: **How does the temperature range inhabited by Crocodylia in deep-time compare to that seen in the present?**

We'll focus our deep-time analysis on the Miocene, a climatically dynamic epoch in Earth's history that can serve as an analog for future climate scenarios (e.g., Steinthorsdottir et al. 2020, https://doi.org/10.1029/2020PA004037).

### Making a Plan

Before we start, it's helpful to write out a step-by-step plan of what we want to accomplish to ensure that the approach we're taking and the code that we write will appropriately answer the question(s) that we are interested in. Think of this as a kind of "logical flow." In other words, what are all the steps needed to get from our research idea to our finalised analysis? This can be as extensive as you wish and can be done in any fashion---even writing out your code or general approach on pen and paper! We'll be doing this a few times today, so let's start by considering our research question and what materials we will need to address it.

As we aim to compare the distributions of Crocodylia in deep-time compared to the present, there are a couple of key datasets that we will require:

1.  A spatially explicit dataset of modern Crocodylia
2.  A spatially explicit dataset of Crocodylia in the deep-time intervals we will assess
3.  A spatially explicit dataset of modern climatic variables
4.  A spatially explicit dataset of climatic variables for the deep-time intervals we choose, likely generated from palaeoclimatic modelling

::: {.callout-note title="Heads Up!"}
Even at this first step, the decisions we make will have consequences for our research and findings. For example, our choice of datasets and how they are handled can impact our results. This will be a recurring theme throughout the session.
:::

## **BREAKOUT SESSION 1**

**Organize yourselves into groups of 4-6 and discuss the following questions:**

***- What issues might we encounter when sourcing, gathering, and working with these various climatic and occurrence datasets?***

***-What caveats should we be aware of, and how can they be addressed?***

## III. Building a Workflow to Merge Datasets

We'll now walk through an example for how we might acquire and harmonize our climatic data. First, let's make a plan of the steps we'll take and some of the decisions we'll need to consider along the way.

1.  **Decide which climate data to use.** What will be most appropriate for the taxonomic group we are studying? Some potential choices might be:

-   The mean annual temperature
-   The mean annual range of temperature
-   The annual standard deviation of temperature
-   The temperature of the hottest or coldest quarter

2.  **Find a suitable source of data.** We will need to find spatially climatic data for each time interval we are interested in and acquire it in an appropriate format (.nc). What factors might we consider?

-   Spatial resolution
-   Temporal resolution

3.  **Sort file structure.** Ensure that your R project is set up appropriately and that the downloaded files are in an appropriate place.

4.  **Load data.** Read our data into R in an appropriate format to work with (e.g., raster). How might we do this? What packages might we need?

5.  **Check formatting.** Ensure that the data are formatted appropriately. How might this impact our results or interpretations? What issues should we be aware of?

-   Do the data have the right extent for our research question and R pipeline, and can this be changed?
-   Are they at an appropriate spatial/temporal resolution, and can this be changed?
-   Are they in the right Coordinate Reference System?
-   Are they capturing the same climatic information?

6.  **Prepare for analysis.** Ensure that the data are ready to be used in our next stages of analysis.To do this, we might want to break down our steps further:

-   Check the format of the data using the 'class()' function
-   Make a reference raster with appropriate resolution and extent
-   Ensure climate raster doesn't have wrapping/extent issues, and rotate if necessary
-   Re-sample raster to our reference raster
-   Ensure that all rasters are capturing the same climatic information

Now that we have a basic plan, we can start to put it into action. Let's write some code!

### 0. Set-up

First, we'll need to install and load some packages that will allow us to work with our data:

```{r}
# Install necessary packages
#install.packages("rgbif")
#install.packages("tidyverse")
#install.packages("ggplot2")
#install.packages("palaeoverse")
#install.packages("raster")
#install.packages("sf")
#install.packages("geodata")
#install.packages("rnaturalearth")
#install.packages("ncdf4")
```

```{r warning=F}
# Load packages
library(rgbif)
library(tidyverse)
library(ggplot2)
library(palaeoverse)
library(raster)
library(sf)
library(geodata)
library(rnaturalearth)
library(ncdf4)
```

```{r}
# Set resolution
res <- 1

# Set extent
e <- extent(-180, 180, -90, 90)

# Make generic raster
r <- raster(res = res, ext = e)
```


### 1. Organising Palaeo-climatic Data

```{r}
# Make function to carry out action faster
prep.raster <- function(file_path, ref.raster){
  # Load .nc file
  raster2prep <- raster(file_path)
  # Rotate data to solve world wrapping issue (if necessary)
  raster.rotated <- raster::rotate(raster2prep)
  # Resample data (the extent and resolution must be updated to avoid rgdal issue)
  raster.resampled <- resample(x = raster.rotated, y = ref.raster)
  # Return data
  return(raster.resampled)
}
```

After exploring several potential palaeo-climatic datasets, we've decided to use a dataset of global mean surface temperatures for the Phanerozoic (Scotese 2022), based on HadleyCM3L simulations and aligned with geochemical proxies like ∂18O. The data are available for 5-million-year intervals that cover our time period of interest (Miocene) and have a 1x1 degree (lat/long) resolution. The dataset is also publically accessible via Zenodo: https://zenodo.org/records/5718392.

Citations:
Scotese, C. R., Song, H., Mills, B. J. W., & van der Meer, D. G. (2021). Phanerozoic paleotemperatures: The earth’s changing climate during the last 540 million years. Earth-Science Reviews, 215, 103503. https://doi.org/10.1016/j.earscirev.2021.103503

Valdes, P.J., Scotese, C.R., and Lunt, D.J. (2021). Deep Ocean Temperatures through Time, Climates of the Past, Discussions, https://doi.org/10.5194/cp-2020-83. 

This climatic dataset offers decently high temporal resolution (for deep time), but one thing we'll need to keep in mind is that the 5-million-year intervals don't align perfectly with geological stages. This will come into play later when we start working with the occurrence data.

For our study, we'll work with two files from this dataset: one covering 25-20 Ma and another covering 20-15 Ma. These data span warming intervals like the Mid Miocene Climatic Optimum and cooling intervals like the Oligocene-Miocene Climate Transition.

```{r}
# Load in our data
# 20-15 Ma interval
scotese15 <- prep.raster(file_path = "015_tas_scotese02a_v21321.nc", 
                         ref.raster = r)

# 25-20 Ma interval
scotese20 <- prep.raster(file_path = "020_tas_scotese02a_v21321.nc", 
                         ref.raster = r)
# Stack rasters
scotese.temp <- stack(scotese15, scotese20)

# Name and plot rasters
names(scotese.temp) <- c("Burdigalian","Aquitanian")
plot(scotese.temp)
```

### 2. Organising Modern Climatic Data

We'll obtain our modern climatic data from WorldClim: https://www.worldclim.org/
To keep our climatic variables consistent, we'll also work with the mean annual temperature data.

NOTE: in here include option for pre-downloaded data

```{r}
# Load data from WorldClim and make into raster
modern <- worldclim_global(var = "tavg", res = 2.5, path = tempdir())
modern <- as(modern, "Raster")

# Calculate mean annual temperature
modern.mean <- calc(x = modern, fun = mean)

# Plot raster
plot(modern.mean)
```

This modern climatic data are considerably higher resolution than what we have available for our deep-time data. What issues might this cause? What actions could we take to resolve this? What could we test?


## **BREAKOUT SESSION 2**

**Now it's your turn to give it a go!**

***-First, take a couple minutes to jot down a workflow for the fossil and modern occurrence data. Think about where you might find these data, if you'll need to do any filtering, and how you might get them in the right format such that we can make fair comparisons across time with the climatic data that we just read into R.***

***-Then, get into the same groups and discuss your approach and where you might encounter potential pitfalls.*** 


### Solution

::: {.callout-tip title="Solution" collapse="true"}

Let's start by thinking about what we need from our occurrence datasets. To answer our question, we aim to compare Crocodylia occurrences in the past and present in relation to temperature data from the same points.

How do we get there? Let's outline some potential steps:

1.  **Decide which occurrence data to use and where to source it.** The PBDB Crocodylia dataset we've been working with contains occurrences that span our interval of interest. Where might we find a comparable dataset of modern occurrences? Earlier today, we learned about the Global Biodiversity Information Facility (GBIF). This could be a good source!

2.  **Filtering.** We'll need to do some filtering, for example to our time period of interest and to remove occurrences that have high uncertainty or aren't comparable between the datasets. What other filtering criteria might we consider?

3.  **Cleaning.** As we learned in the previous session, we'll also want to spend some time cleaning and exploring our data.

4.  **Merging data.** We'll next want to combine our occurrence and climatic datasets to extract the mean temperature values at the same points in space and time as the occurrences. When doing this, what are some factors we need to consider? This might involving binning our data given differences in temporal and spatial resolutions and making some decisions about which taxonomic level to run our analysis. Along the way, we'll likely need to think carefully about the dataframe structures and identify common data fields that are relevant for our study (time designations, taxonomy, coordinates, collection numbers, etc). We'll also want to consider potential sampling biases that might influence our comparisons.

5. **Data extraction and visualisation.** At this point, we'll have binned fossil and modern occurrences with geographic coordinates and associated temperature data. Returning to our question, this will allow us to determine whether the fossil occurrences cover a broader range of temperatures than the modern occurrences.
:::

Let's begin!

### 1. Sourcing occurrence data

#### 1A. Organise fossil occurrence data

First, we'll load in our palaeontological data. Luckily for us, we've already been working with a Crocodylia fossil occurrence dataset. We'll use these data in our analysis. If needed, you can read in the cleaned dataset below.
```{r}
# Load cleaned fossil data for order Crocodylia
fossil_occ <- read.csv("crocs_fossil_test.csv")
```

#### 1B. Organise modern occurrence data

After looking at the number and coverage of available records, we've decided to use the GBIF data.

We'll start by doing a quick data download using the `rgbif` package for the purposes of this exploratory exercise. Note that this method for retrieving occurrences has some limitations, and we'd want to do a formal download query before running our full analysis for publication. For that, you'd need to make a GBIF account.

Here's how we can do this. Scroll down to load the dataset we've previously downloaded.
```{r}
# Set taxa key to order Crocodylia
#crocodile_key <- name_backbone(name = "Crocodylia")$usageKey

# Download data
#modern_download <- occ_search(
  #taxonKey = crocodile_key,
  #hasCoordinate = TRUE, #needs to have coordinates
  #basisOfRecord = "PRESERVED_SPECIMEN", #keeping just preserved specimens for better comparability with the PBDB data
  #limit = 10000)  #adjust limit (if you set limit to 0, you can check the metadata to see how many records are available)

# Extract data
#modern_occ <- modern_download$data

# Extract metadata
#modern_meta <- modern_download$meta
```

For ease, here's a csv of the raw data:
```{r warning=F}
# Load modern data
modern_occ <- read.csv("crocs_modern.csv") 
```


### 2. Filtering

These datasets contain occurrences that are relevant to our research question, but they also contain occurrences that fall outside of the scope of our planned analysis. Let's filter the datasets to just keep the relevant occurrences. What were some of the filtering criteria you brainstormed during the breakout session?

**-Data type:** Note that we've already queried GBIF for just the preserved specimens to optimize comparability with the PBDB dataset. For instance, GBIF also contains fossil specimens, living specimens, and human observations that either aren't entirely comparable with a fossil occurrence or don't represent our modern time interval.

**-Taxonomic:** We'll run our analysis at the order level (Crocodylia) given taxonomic differences between the fossil and modern datasets at finer taxonomic levels (e.g., extinct taxa). That said, let's just retain occurrences that have been identified at least to genus (i.e., removing indet. taxa from the fossil dataset) given uncertainty in their taxonomy.

**-Spatial:** Our research question is posed at a global scale, so we can keep everything. However, we will still want to check for potential geographic outliers or inconsistencies while cleaning the data.

**-Temporal:** We'll constrain our palaeontological dataset to the same time intervals as our climatic dataset (25-15 Ma). Earlier, we downloaded Crocodylia occurrences for the whole of the Cenozoic, so we'll need to do some filtering. Our modern dataset includes occurrences from the 1800s to today, whereas the WorldClim dataset starts at 1970, so we can also filter the occurrence dataset to that time range for consistency.


```{r}
## PBDB filtering
fossil_occ_filter <- fossil_occ %>%
  dplyr::filter(accepted_rank == "genus" | accepted_rank =="species") %>% #taxonomic filtering
  dplyr::filter(max_ma<=25 & min_ma>=15) #temporal filtering

## GBIF filtering
modern_occ_filter <- modern_occ %>%
  filter(!is.na(genus)) %>% #taxonomic filtering
  filter(year >= 1970) %>% #temporal filtering
  filter(coordinateUncertaintyInMeters < 10000 | is.na(coordinateUncertaintyInMeters)) #we could also remove data with uncertain geographic coordinates
```

This filtering step could have removed a fair amount of data. How many occurrences remain in each filtered dataset?
```{r}
nrow(fossil_occ_filter)
nrow(modern_occ_filter)
```

After filtering, we have 114 fossil occurrences and 2504 modern occurrences to potentially work with. How might we deal with this unequal sampling?


### 3. Cleaning

We'll then want to explore and clean our data to check for outliers, inconsistencies, duplicates, and other potential issues.

Our fossil occurrence dataset has already gone through this process.

**Let's spend a couple of minutes cleaning and familiarising ourselves with the GBIF data, applying what we learned in the previous session.** For example, you might look at the range of latitudes represented by occurrences or check the taxa for spelling errors. You could plot them on a map to see if you notice any geographic oddities. Or you could check for duplicates.

```{r}
# Practice cleaning the modern occurrence dataset here!
```


### 4. Merging

Now we come to the fun part. How do we align these data, which were sampled in different ways with different temporal and spatial resolutions and coverage? Note that the header names differ, and that the datasets contain variables reported in different ways (units, formats) and with different resolutions.

## **BREAKOUT SESSION 3**

**Return to your discussion groups and discuss the following questions:**

***-What factors that might introduce bias when we try to make comparisons across these datasets?***

***-What are some sources of uncertainty?***

***-How might we address these potential confounds prior to analysis and/or what should we keep in mind as we're interpreting our results?***

::: {.callout-tip collapse="true"}
The resolution and coverage of the data is likely one aspect that came up in your group discussions. As we move forward, we'll need to make some practical and philosophical decisions about how we bin the data given their varying resolutions. We'll also want to consider the sensitivity of our results to these various decisions. 
:::

Here, we'll touch on several key taxonomic, temporal, and spatial considerations when endeavoring to make these datasets comparable.

#### Taxonomy

We'll be using higher-level taxonomic groups for our analysis. Let's take a look to see what's here.
```{r}
# Fossil occurrences
table(fossil_occ_filter$family)

# Modern occurrences
table(modern_occ_filter$family)
```

We see that the same families are represented in our datasets. If you were to look at the genera, you'll notice some differences. If we wanted to run an analysis at a finer taxonomic level, what would be some key considerations or steps that we would need to take?

#### Temporal Bins

Earlier, we noticed that our climatic data were available at 5-million-year intervals, which don't align perfectly with geological stages. For example, our 25-20 Ma interval crosses the Chattian and Aquitanian stages and the 20-15 Ma interval crosses the Burdigalian and Langhian stages. Each fossil occurrence has an associated age and interval range. We want to create time bins that roughly correspond with our climate layer so we can look at occurrences in relation to the temperature reconstructions. How might we approach this?

First, let's take a look at the temporal distribution of our fossil occurrences.
```{r}
tax_range_time(occdf = fossil_occ_filter, 
               name = "occurrence_no",
               min_ma = "min_ma", 
               max_ma = "max_ma", 
               plot = TRUE)
axis_geo(intervals = "stages")
```

One option would be to bin the data by stage and use those as proxies for our 5-million-year climatic intervals.
Another approach might be to make near-equal-length bins within the 25-15 Ma range, and then assign occurrences to those bins. Let's try that here. 
```{r}
# Generate time bins of ~5 million years within 25 and 15 Ma
bins <- time_bins(interval = c(15,25), 
                  rank = "stage",
                  size = 5, #here's where we specify the equal-length bins to approximate the resolution of our climatic data
                  scale = "GTS2020",
                  plot = TRUE)
# Check bins
head(bins)
```

Additionally, there are a couple methods that we could use to bin the occurrences. Let's test out two approaches here to see how they differ. Hopefully you're starting to get a sense of how these various decisions might propagate to affect our results.
```{r}
# `mid` method = uses the midpoint of age range to bin the occurrence
fossil_occ_mid <- bin_time(occdf = fossil_occ_filter,
                      bins = bins,
                      method = 'mid')

# `majority` method = bins an occurrence into the bin which it most overlaps with
fossil_occ_maj <- bin_time(occdf = fossil_occ_filter,
                      bins = bins,
                      method = 'majority') 

# Compare the bin assignments
bin_comparison <- fossil_occ_mid[c('occurrence_no','bin_assignment','n_bins')] %>% 
  left_join(fossil_occ_maj[c('occurrence_no','bin_assignment','n_bins')],by='occurrence_no') %>% 
  mutate(agreement = case_when(
    bin_assignment.x == bin_assignment.y ~ "yes"
  ))

```

In this case, they actually match up! Since these approaches yielded the same results, we'll use the 'maj' dataset moving forward.
```{r}
# Renaming
fossil_occ_bin <- fossil_occ_maj

# How many occurrences per bin?
table(fossil_occ_bin$bin_assignment)
```

We see that most of the data falls into bin 3 (20.44-13.82 Ma, midpoint=17.130 Ma), with some occurrences in bin 2 (23.03-20.44 Ma, midpoint=21.735 Ma).

-  Bin 2 overlaps with our 25-20 Ma climate raster
-  Bin 3 overlaps with our 20-15 Ma climate raster

Our GBIF dataset already contains just modern data, which overlaps with our WorldClim climate layer, so no additional binning is needed. 

OK, so now we have some temporal bins to facilitate our comparisons with the climatic data.


#### Spatial Bins

We also observed that the spatial resolution of the climatic data differed between the past and present. Additionally, the uncertainties in the coordinates for our modern and fossil occurrences differ, especially since we'll need to rely on plate models to palaeorotate the geographic positions of our fossil occurrences. How might we deal with this?

Like before, let's take a look at the spatial distribution of our fossil occurrences.
```{r}
# Load in a world map
world <- ne_countries(scale = "small", returnclass = "sf")

# Plot the geographic coordinates of each locality over the world map
ggplot(fossil_occ_bin) +
  geom_sf(data = world) +
  geom_point(aes(x = lng, y = lat)) +
  labs(x = "Longitude (º)",
       y = "Latitude (º)")+ 
  theme_void()
```

But wait, remember that we need to paleorotate our coordinates before we proceed. We could run a couple different plate models to check the sensitivity.
```{r}
# Palaeorotation
fossil_occ_bin_rot <- palaeorotate(fossil_occ_bin,
                                   lng = "lng",
                                   lat = "lat",
                                   age = "bin_midpoint", 
                                   model  = "PALEOMAP")

# Remove NAs
fossil_occ_bin_rot <- fossil_occ_bin_rot %>%
  dplyr::filter(!is.na(p_lng) & !is.na(p_lat))

# Plot palaeorotated coordinates
world <- ne_countries(scale = "small", returnclass = "sf")
ggplot(fossil_occ_bin_rot) +
  geom_sf(data = world) +
  geom_point(aes(x = p_lng, y = p_lat)) +
  labs(x = "Paleolongitude (º)",
       y = "Paleolatitude (º)")+ 
  theme_void()
```

For comparison, let's also plot the modern occurrences.
```{r}
# Load in a world map
world <- ne_countries(scale = "small", returnclass = "sf")

# Plot the geographic coordinates of each locality over the world map
ggplot(modern_occ_filter) +
  geom_sf(data = world) +
  geom_point(aes(x = decimalLongitude, y = decimalLatitude)) +
  labs(x = "Longitude (º)",
       y = "Latitude (º)")+ 
  theme_void()
```

Looking at these spatial representations of the data, we can really visualise the difference in sampling effort. How might we account for the spatial structure of the data in our analysis?

Another challenge stems from aligning these occurrences with the spatial explicit climate layers. A simple approach would be to extract the mean temperature value at each coordinate. Alternatively, we might consider adding a bit of a buffer around each coordinate to incorporate geographic uncertainty and variation in the climatic data. A third, albeit more complex approach, would be to assign the occurrences to spatial bins so we can match them up to the climate layers (e.g., check out the `bin_space` function). We'd also need to resolve the difference the spatial resolutions between the past and present climatic data, potentially by coarsening the modern dataset.

We'll keep things simple and show the first approach here.


## IV. Data Extraction and Visualisation

We have our data binned to match the climate layers, so let's create separate datasets for each time bin.
```{r}
# 25-20 Ma climate interval
points_20 <- fossil_occ_bin_rot %>%
  dplyr::filter(bin_assignment == 2)

# 20-15 Ma climate interval
points_15 <- fossil_occ_bin_rot %>%
  dplyr::filter(bin_assignment == 3) 

# Modern interval (whole dataset)
points_mod <- modern_occ_filter
```

Next, let's set up our points and ensure they're on the same coordinate system. Recall that this was one of the things we wanted to check when we outlined our workflow for the climatic data at the beginning of the session.
```{r}
# Turn points into spatial features
points_sf_20 <- st_as_sf(x = points_20, coords = c("p_lng", "p_lat"))
points_sf_15 <- st_as_sf(x = points_15, coords = c("p_lng", "p_lat"))
points_sf_mod <- st_as_sf(x = points_mod, coords = c("decimalLongitude", "decimalLatitude"))

# Set coordinate system (WGS84) for occurrences
st_crs(points_sf_20) <- st_crs(4326)
st_crs(points_sf_15) <- st_crs(4326)
st_crs(points_sf_mod) <- st_crs(4326)
```

We're now ready to extract the climatic data at each coordinate!
```{r}
# Past temperature (Scotese 2022)
scotese_temp_20 <- raster::extract(x = scotese.temp$Aquitanian, 
                           y = points_sf_20, 
                           sp = TRUE, 
                           cellnumbers = TRUE)

scotese_temp_15 <- raster::extract(x = scotese.temp$Burdigalian, 
                                     y = points_sf_15, 
                                     sp = TRUE, 
                                     cellnumbers = TRUE)

# Modern temperature (WorldClim)
modern_temp <- raster::extract(x = modern.mean, 
                                     y = points_sf_mod, 
                                     sp = TRUE, 
                                     cellnumbers = TRUE)
```

Let's take a look. How does the temperature range inhabited by Crocodylia in deep-time compare to that seen in the present?
```{r}
# Aquitanian range (25-20 Ma)
range(scotese_temp_20$Aquitanian, na.rm=T)
diff(range(scotese_temp_20$Aquitanian, na.rm=T))

# Burdigalian/Langhian range (20-15 Ma)
range(scotese_temp_15$Burdigalian, na.rm=T)
diff(range(scotese_temp_15$Burdigalian, na.rm=T))

# Modern range
range(modern_temp@data$layer, na.rm=T)
diff(range(modern_temp@data$layer, na.rm=T))
```

Finally, let's plot the data.
```{r}
# Organise data for plotting
palaeo_temp <- rbind(data.frame(Bin = rep("Burdigalian", length(scotese_temp_15$Burdigalian)), #
                           Temperature = scotese_temp_15$Burdigalian), 
                      data.frame(Bin = rep("Aquitanian", length(scotese_temp_20$Aquitanian)), 
                                  Temperature = scotese_temp_20$Aquitanian),
                      data.frame(Bin = rep("Modern", length(modern_temp@data$layer)), 
                                  Temperature = modern_temp@data$layer))

# Make box plot of temperature
ggplot(palaeo_temp, aes(x = Bin, y = Temperature)) +
  geom_jitter(color="gray80",size=2,alpha=0.5)+
  geom_boxplot(width=0.5,alpha=0.7,outlier.shape = NA)+
  labs(x="Time interval", y="Temperature (ºC)")+
  theme_bw()
```

When looking at the better sampled intervals in particular, we find that the mean temperature range of Crocodylia occurrences was quite similar between the Miocene and Modern.
How might we interpret our results? How might this interpretation be shaped by the caveats we pointed out along the way?

Ultimately, our goal was to provide a workflow for merging different types of datasets and thinking critically about some of the caveats that arise when doing so. These taxonomic, spatial, and temporal considerations are critical to address and report in palaeobiological and palaeoecological research in general---but particularly in studies aiming to cross the palaeontological-ecological gap. We hope you can adapt and learn from this script as you venture forward with your own exciting reserach questions and datasets!
