---
title: "Data Processing I"
subtitle: "Data Processing I: Data Exploration and Cleaning"
toc: true
order: 4
execute:
  enabled: false
  code-fold: true
---

# Data processing I: Data exploration and cleaning

**Aim:**

- Understand why data exploration and cleaning is key for data analyses
- Develop the skills and knowledge needed to explore and clean data

**Schedule:** 11:15-12:30 [1.25 hrs]

# Data exploration

- Why do we explore our data? How do we explore our data? [20 min instruction & code-along, Will]
Based on https://tenrules.palaeoverse.org/#rule-4-explore-your-data
Guided freeform practical [15 min, Will]

# Data cleaning

## Identify and handle incomplete data records

Datasets are rarely perfect. A common issue you may encounter when exploring your data is ambiguous, incomplete, or missing data entries. These incomplete or missing data records can occur due to various reasons. In some cases, the data truly do not exist or cannot be estimated due to issues relating to taphonomy, collection approaches, or biases in the fossil record. In other cases, discrepancies may arise because data were collected when definitions or contexts differed, such as shifts in geopolitical boundaries and country names over time. Additionally, data may be incomplete for some records, but can be inferred through other available data. 

## Why is it important?

Missing information can bias the results of palaeobiological studies. Occurrence data are inherently based on the existence of a particular fossil, but missing data associated with that fossil occurrence can also affect analyses that rely on that associated data. For instance, missing temporal or spatial data may prevent you from including occurrences in your analyses.

## What should we do with incomplete data records?

Depending on your research goals, incomplete entries may either be removed through filtering or addressed through imputation techniques. Data imputation approaches can be used to replace missing data with values modelled on the observed data using various methods. These can range from simple approaches, like replacing missing values with the mean for continuous variables, to more advanced statistical or machine learning techniques. If you do decide to impute missing data, it is essential that this process and its effects on the dataset are clearly justified and documented so that future users of the dataset or analytical results are aware of these decisions. Although missing data can reduce the statistical power of analyses and bias the results, imputing missing values can introduce new biases, potentially also skewing results and interpretations of the examined data.

To decide how to handle missing data, start by identifying the gaps in your dataset, which are often represented by empty entries or ‘NA’. For imputing missing values, numerous methods and tools are available in your coding language of choice, such as missForest, mice, and kNN. Removing missing data can be straightforward when working with small datasets. For manual removal, tools such as spreadsheet software can be sufficient. In R, built-in functions such as `complete.cases()` and `na.omit()` quickly identify and remove missing values (note: this will remove complete whole rows). The tidyr package also provides the `drop_na()` function for this purpose.

### Identifying and handling incomplete data records

By default, when we read data tables into R, it recognises empty cells and takes some course of action to manage them. When we use base R functions, such as `read.csv()`, empty cells are given an NA value (‘not available’) only when the column is considered to contain numerical data. When we use Tidyverse functions, such as `readr::read_csv()`, all empty cells are given NA values. This is important to bear in mind when we want to find those missing values: here, we have done the latter, so all empty cells are NA.

The extent of incompleteness of the different columns in our dataset is highly variable. For example, the number of NA values for the collection_no is 0.

```{r}
# Count the number of collection number values for which `is.na()` is TRUE
sum(is.na(fossils$collection_no))
```

This is because it is impossible to add an occurrence to the PBDB without putting it in a collection, which must in turn have an identification number.

However, what about genus?

```{r}
# Count the number of genus IDs for which `is.na()` is TRUE
sum(is.na(fossils$genus))
```

What other columns might we want to check?

```{r}
# Latitude
sum(is.na(fossils$lat))
```

```{r}
# Palaeolatitude
sum(is.na(fossils$paleolat))
```

```{r}
# Geological formation
sum(is.na(fossils$formation))
```

```{r}
# Country code
sum(is.na(fossils$cc))
```

### Handling incomplete data records

OK, so we've identified some incomplete data records, what do we do now?

- Filter
- Impute
- Complete

#### Filter

While all occurrences have modern day coordinates, some are missing palaeocoordinates. We will now remove these occurrences from the dataset.

```{r}
# Remove occurrences which are missing palaeocoordinates
fossils <- filter(fossils, !is.na(fossils$paleolng))

# Check whether this has worked
sum(is.na(fossils$paleolat))
```

A further option applicable in some cases would be to fill in our missing data. We may be able to interpolate values from the rest of our data, or use additional data sources. For our palaeogeography example above, we could generate our own coordinates, for example using `palaeoverse::palaeorotate()`.

#### Impute

Data imputation is the process of replacing missing values in a dataset with substituted values. How might we do this for our formation names?

- We could estimate potential formations by using geographic coordinates to extract formations from a geological map. 
- We could evaluate whether any nearby collections of the same age have associated formation names.
- Ultimately we should trace back to the original literature and try to resolve this issue more robustly if the source material allows.

#### Complete

::: {.callout-important}

## A word of caution

We identified several data records without country codes. We could quickly filter this data, it's not that much data after all. But you've just remembered something! The country where the collection is located is a compulsory entry field in the PBDB! **What on Earth has gone wrong?**

:::

::: {.callout-tip collapse="TRUE"}

## Answer

Any guesses on what the country code for **NA**mibia is?

R has interpreted Namibia's country code as a 'NA' value. 

This is an important illustration of why we should conduct further investigation when any apparent errors arise in the dataset, rather than immediately removing these data points.

:::

## Identify and handle outliers [Lewis]


## Identify and handle inconsistencies [Pedro]


## Identify and handle duplicates [Pedro]

# Resources




